{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the description formatted in Markdown:\n",
    "\n",
    "# IMF Publication Data Scraping Script\n",
    "\n",
    "## Description\n",
    "\n",
    "This Python script automates the process of scraping publication data from the International Monetary Fund (IMF) website. By utilizing the IMF's API, it collects metadata about publications and downloads available documents. The script is designed to handle network issues robustly and efficiently, storing results in a format that facilitates data analysis and archival.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Session Management with Retry**\n",
    "  - Uses a `requests.Session` with a retry strategy to manage transient errors such as timeouts or server errors, increasing the robustness of the scraping process.\n",
    "\n",
    "- **Progress Tracking**\n",
    "  - Utilizes `tqdm` to display a progress bar, providing visual feedback during the scraping process and enhancing user interaction.\n",
    "\n",
    "- **Data Extraction**\n",
    "  - Parses JSON responses from the IMF API to extract relevant publication metadata.\n",
    "  - Scrapes individual article pages to locate and download publication files.\n",
    "\n",
    "- **Data Storage**\n",
    "  - Downloads publication files and saves them locally.\n",
    "  - Stores metadata in a JSON format, allowing for organized storage and easy retrieval for future analysis.\n",
    "\n",
    "You can copy and paste this Markdown text into any Markdown editor or viewer to see the formatted output. If you have any additional requests or need further modifications, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Base URL for the IMF API\n",
    "base_url = \"https://www.imf.org/api/imf/countrysearch/search\"\n",
    "\n",
    "# Initialize a list to store all results\n",
    "all_results = []\n",
    "\n",
    "# Create a directory named 'save' if it does not exist\n",
    "save_directory = \"save\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Create a session object and set up a retry strategy\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=5,  # Total number of retry attempts\n",
    "    backoff_factor=1,  # Time delay between retries\n",
    "    status_forcelist=[429, 500, 502, 503, 504],  # HTTP status codes that trigger a retry\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]  # Methods allowed to retry\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "\n",
    "# Total number of pages to scrape ,Total 371\n",
    "total_pages = 20\n",
    "\n",
    "# Use tqdm to add a progress bar\n",
    "for page in tqdm(range(1, total_pages + 1), desc=\"Scraping Pages\", unit=\"page\"):\n",
    "    # Define parameters for the API request\n",
    "    params = {\n",
    "        \"language\": \"en\",\n",
    "        \"country\": \"0f4439dc-b751-4b90-9c3a-b9923544c3da\",\n",
    "        \"selectedFilters\": \"Publications\",\n",
    "        \"currentPage\": page,\n",
    "        \"pageSize\": 10,\n",
    "        \"excludeItemId\": \"89af01a4-70e0-498e-8edd-4d7c9946d752\"\n",
    "    }\n",
    "\n",
    "    # Send a GET request to retrieve the response\n",
    "    try:\n",
    "        response = session.get(base_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page {page}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract title, date, URL, and tags from each result\n",
    "        for item in data['results']:\n",
    "            title = item['title']\n",
    "            date = item['date']\n",
    "            url = item['url']\n",
    "            tags = item['tags']\n",
    "\n",
    "            # Initialize file name as an empty string\n",
    "            file_name = \"\"\n",
    "\n",
    "            # Download files from each article page\n",
    "            article_url = f\"https://www.imf.org{url}\"  # Ensure URL is constructed correctly\n",
    "            try:\n",
    "                article_response = session.get(article_url, timeout=10)\n",
    "                article_response.raise_for_status()\n",
    "                soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "                download_link = soup.select_one('.piwik_download')\n",
    "\n",
    "                if download_link and download_link.get('href'):\n",
    "                    download_url = f\"https://www.imf.org{download_link.get('href')}\"  # Ensure download URL is correct\n",
    "                    file_name = download_url.split('/')[-1]\n",
    "                    file_path = os.path.join(save_directory, file_name)\n",
    "\n",
    "                    # Send GET request to download file\n",
    "                    file_response = session.get(download_url, timeout=10)\n",
    "                    file_response.raise_for_status()\n",
    "                    # Save file locally\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(file_response.content)\n",
    "                    print(f\"Downloaded: {file_name}\")\n",
    "                else:\n",
    "                    print(f\"No download link found for article: {title}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching article {title}: {e}\")\n",
    "\n",
    "            # Append extracted information and file name to results list\n",
    "            all_results.append({\n",
    "                \"title\": title,\n",
    "                \"date\": date,\n",
    "                \"url\": f\"https://www.imf.org{url}\",\n",
    "                \"tags\": tags,\n",
    "                \"filename\": file_name[:-4] + \"txt\"  # Add file name to result\n",
    "            })\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for page {page}\")\n",
    "\n",
    "# Print or save all results to a file\n",
    "json_path = os.path.join(save_directory, 'imf_data8.json') # The file is named imf data+ numbers +.json\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Total articles scraped: {len(all_results)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Text Extraction and Cleaning Script\n",
    "\n",
    "## Description\n",
    "\n",
    "This script processes `.ashx` files, converts them to PDFs, extracts text, and saves the cleaned text into separate text files. It is particularly useful for automating the extraction of text from PDF files generated from `.ashx` files, which can be cumbersome if done manually, especially in bulk processing tasks.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **PDF Extraction and Cleaning**\n",
    "  - The script extracts text from PDF files using `PyPDF2` and cleans the extracted text to remove unwanted characters and patterns. This ensures that the text is in a readable format.\n",
    "\n",
    "- **File Handling**\n",
    "  - Converts `.ashx` files to `.pdf` files by copying and renaming them. It then processes these PDF files, extracting and cleaning the text, and saves the results in a specified directory.\n",
    "\n",
    "- **Directory Management**\n",
    "  - Automatically creates directories for storing converted PDFs and cleaned text files if they do not exist, ensuring the script runs smoothly even if these directories are not pre-created.\n",
    "\n",
    "- **Error Handling**\n",
    "  - Includes error handling to manage exceptions that might occur during file reading, writing, or text extraction, providing informative messages for debugging purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    :param file_path: The path to the PDF file.\n",
    "    :return: Extracted text or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create a PDF reader object\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            # Initialize a variable to store extracted text\n",
    "            text = ''\n",
    "            # Loop through each page\n",
    "            for page in reader.pages:\n",
    "                # Extract text and append to the string\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans extracted text by removing unwanted characters and patterns.\n",
    "\n",
    "    :param text: The raw extracted text.\n",
    "    :return: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove unwanted characters or patterns\n",
    "    text = re.sub(r'\\x0c', '', text)  # Remove form feed characters\n",
    "    text = re.sub(r'[\\r\\n]+', '\\n', text)  # Normalize newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces to single spaces\n",
    "    text = re.sub(r'\\s*-\\s*\\n', '', text)  # Handle hyphenated line breaks\n",
    "    text = re.sub(r'_', '', text)  # Remove underscores\n",
    "    text = re.sub(r'\\�', '', text)  # Remove replacement character\n",
    "    text = re.sub(r'\\. ', '', text)  # Remove misplaced periods\n",
    "    text = re.sub(r'\\\b', '', text)  # Remove backspace characters\n",
    "    text = re.sub(r'\\.{2}', '', text)  # Remove two consecutive periods\n",
    "    text = re.sub(r'\\-{2}', '', text)  # Remove two consecutive hyphens\n",
    "    text = re.sub(r'[\\x0c\\x0e]', '', text)  # Remove form feed and \\x0e characters\n",
    "    # Remove known headers or footers\n",
    "    text = re.sub(r'INTERNATIONAL MONETARY FUND', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bPage\\s*\\d+\\b', '', text, flags=re.IGNORECASE)  # Remove \"Page\" numbering\n",
    "\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', '', text)  # Remove ASCII non-printable characters\n",
    "    return text.strip()\n",
    "\n",
    "def save_text_to_file(text, output_path):\n",
    "    \"\"\"\n",
    "    Saves cleaned text to a new text file.\n",
    "\n",
    "    :param text: The cleaned text.\n",
    "    :param output_path: Path where the text file will be saved.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "def process_pdf(file_path, output_text_path):\n",
    "    \"\"\"\n",
    "    Extracts and cleans text from a PDF, then saves it to a text file.\n",
    "\n",
    "    :param file_path: Path to the PDF file.\n",
    "    :param output_text_path: Path where the cleaned text will be saved.\n",
    "    :return: The path to the saved text file, or None if extraction fails.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF\n",
    "    extracted_text = extract_text_from_pdf(file_path)\n",
    "    if extracted_text is not None:\n",
    "        # Clean the extracted text\n",
    "        cleaned_text = clean_text(extracted_text)\n",
    "        # Save the cleaned text to a file\n",
    "        save_text_to_file(cleaned_text, output_text_path)\n",
    "        return output_text_path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Directory paths\n",
    "input_directory = 'save'\n",
    "pdf_directory = 'savepdf'\n",
    "text_directory = 'saveText'\n",
    "\n",
    "# Create target directories if they do not exist\n",
    "os.makedirs(pdf_directory, exist_ok=True)\n",
    "os.makedirs(text_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over all .ashx files in the save directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.ashx'):\n",
    "        # Source file path\n",
    "        source_file_path = os.path.join(input_directory, filename)\n",
    "        # Force conversion to .pdf file path\n",
    "        pdf_file_name = os.path.splitext(filename)[0] + '.pdf'\n",
    "        pdf_file_path = os.path.join(pdf_directory, pdf_file_name)\n",
    "\n",
    "        # Copy the file and rename it as .pdf\n",
    "        shutil.copy(source_file_path, pdf_file_path)\n",
    "        print(f\"Converted to PDF file: {pdf_file_path}\")\n",
    "\n",
    "        # Output text file path\n",
    "        text_file_name = os.path.splitext(pdf_file_name)[0] + '.txt'\n",
    "        text_file_path = os.path.join(text_directory, text_file_name)\n",
    "\n",
    "        # Process the PDF and save the cleaned text\n",
    "        try:\n",
    "            result = process_pdf(pdf_file_path, text_file_path)\n",
    "            if result:\n",
    "                print(f\"Cleaned text saved to {text_file_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to extract text from PDF file: {pdf_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF file {pdf_file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMF Publication Metadata Processing Script\n",
    "\n",
    "## Description\n",
    "\n",
    "This script processes JSON files containing publication metadata from the International Monetary Fund (IMF) and segregates entries based on whether they have a valid filename field. The results are saved as both JSON and Excel files for easy access and analysis.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **File Processing**\n",
    "  - Uses `glob` to find all JSON files matching the pattern `imf_data*.json`, allowing it to process multiple files in one run.\n",
    "\n",
    "- **Data Segregation**\n",
    "  - **With Filename**: Entries with a non-empty filename field that is not just `'txt'`.\n",
    "  - **Without Filename**: Entries missing the filename field or having it set to an empty string or `'txt'`.\n",
    "\n",
    "- **Data Output**\n",
    "  - The results are saved into both JSON and Excel formats, providing flexible options for further analysis or reporting:\n",
    "    - **JSON Files**: `with_filename.json` and `without_filename.json` store the segregated data.\n",
    "    - **Excel Files**: `with_filename.xlsx` and `without_filename.xlsx` provide a tabular view, which is especially useful for data analysis using spreadsheet software.\n",
    "\n",
    "- **DataFrame Conversion**\n",
    "  - By converting the lists to Pandas DataFrames, the script can easily export the data to Excel format, which is suitable for handling tabular data and conducting further analyses.\n",
    "\n",
    "This script efficiently categorizes and exports IMF publication metadata, allowing users to easily handle and analyze the data in both JSON and Excel formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "with_filename = []\n",
    "without_filename = []\n",
    "\n",
    "# Iterate over all JSON files matching the pattern 'imf_data*.json'\n",
    "for filename in glob.glob('imf_data*.json'):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            # Check if the entry has a 'filename' field that is not empty and not equal to 'txt'\n",
    "            if 'filename' in entry and entry['filename'] and entry['filename'] != 'txt':\n",
    "                with_filename.append(entry)\n",
    "            else:\n",
    "                without_filename.append(entry)\n",
    "\n",
    "# Define output JSON filenames\n",
    "json_with_filename = 'with_filename.json'\n",
    "json_without_filename = 'without_filename.json'\n",
    "\n",
    "# Save entries with valid filenames to a JSON file\n",
    "with open(json_with_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(with_filename, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Created JSON file with valid filenames: {json_with_filename}\")\n",
    "\n",
    "# Save entries without valid filenames to another JSON file\n",
    "with open(json_without_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(without_filename, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Created JSON file without valid filenames: {json_without_filename}\")\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_with_filename = pd.DataFrame(with_filename)\n",
    "df_without_filename = pd.DataFrame(without_filename)\n",
    "\n",
    "# Define output Excel filenames\n",
    "excel_with_filename = 'with_filename.xlsx'\n",
    "excel_without_filename = 'without_filename.xlsx'\n",
    "\n",
    "# Export entries with valid filenames to an Excel file\n",
    "df_with_filename.to_excel(excel_with_filename, index=False)\n",
    "print(f\"Created Excel file with valid filenames: {excel_with_filename}\")\n",
    "\n",
    "# Export entries without valid filenames to another Excel file\n",
    "df_without_filename.to_excel(excel_without_filename, index=False)\n",
    "print(f\"Created Excel file without valid filenames: {excel_without_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Data Processing Script\n",
    "Description\n",
    "\n",
    "This script processes data from a JSON file, segregates the entries based on a specific condition regarding the filename field, and exports the separated data into both JSON and Excel formats. This allows for easy data management and analysis.\n",
    "Key Features\n",
    "\n",
    "    Loading JSON Data: The script reads data from a specified JSON file.\n",
    "    Data Separation: It separates the entries based on whether the filename field equals \"indextxt\".\n",
    "    Exporting Data: The separated data is then saved into separate JSON and Excel files for further analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data from the specified file\n",
    "file_path = 'with_filename.json'  # Path to your JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)  # Load JSON data\n",
    "\n",
    "# Separate the entries based on the condition for filename\n",
    "# Filter entries where the filename is \"indextxt\"\n",
    "to_remove = [entry for entry in data if entry.get('filename') == \"indextxt\"]\n",
    "# Get the remaining entries\n",
    "remaining_data = [entry for entry in data if entry.get('filename') != \"indextxt\"]\n",
    "\n",
    "# Export the separated data to JSON files\n",
    "removed_json_path = 'removed_entries.json'  # Path for JSON file of removed entries\n",
    "remaining_json_path = 'remaining_entries.json'  # Path for JSON file of remaining entries\n",
    "\n",
    "# Save removed entries to JSON file\n",
    "with open(removed_json_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(to_remove, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Save remaining entries to JSON file\n",
    "with open(remaining_json_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(remaining_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Convert the data to DataFrames and save as Excel files\n",
    "removed_df = pd.DataFrame(to_remove)  # Convert removed entries to DataFrame\n",
    "remaining_df = pd.DataFrame(remaining_data)  # Convert remaining entries to DataFrame\n",
    "\n",
    "removed_excel_path = 'removed_entries.xlsx'  # Path for Excel file of removed entries\n",
    "remaining_excel_path = 'remaining_entries.xlsx'  # Path for Excel file of remaining entries\n",
    "\n",
    "# Save removed entries to Excel file\n",
    "removed_df.to_excel(removed_excel_path, index=False)\n",
    "\n",
    "# Save remaining entries to Excel file\n",
    "remaining_df.to_excel(remaining_excel_path, index=False)\n",
    "\n",
    "# Print the paths of the saved files\n",
    "print(f\"Removed entries saved to JSON: {removed_json_path}\")\n",
    "print(f\"Remaining entries saved to JSON: {remaining_json_path}\")\n",
    "print(f\"Removed entries saved to Excel: {removed_excel_path}\")\n",
    "print(f\"Remaining entries saved to Excel: {remaining_excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
